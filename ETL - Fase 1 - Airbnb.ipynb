{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fase 1 - Extract**\n",
    "*En esta fase inicial, el enfoque está en la extracción de datos brutos directamente desde la plataforma de Airbnb. Utilizando un scraper automatizado, extraemos información esencial sobre las propiedades disponibles, como ubicación, precios, tipo de alojamiento, disponibilidad y reseñas de los usuarios. La extracción es un paso crucial para asegurar que todos los datos relevantes se reúnan y estén listos para su análisis. Nos aseguramos de que los datos se extraigan de forma precisa y consistente, lo cual es fundamental para el éxito de las fases siguientes del proceso. El objetivo de esta fase es consolidar la información extraída en un formato accesible (DataFrame) de manera eficiente y automatizada, para que posteriormente puedan ser limpiados y transformados.* **(necesita edicion - version test)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL imports\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from time import sleep\n",
    "\n",
    "# General \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json          \n",
    "import os\n",
    "import tqdm\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = Options()\n",
    "opts.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\n",
    "    service=Service(ChromeDriverManager().install()),\n",
    "    options=opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de distritos de Barcelona\n",
    "distritos = [\n",
    "    \"Ciutat Vella\",\n",
    "    \"Eixample\",\n",
    "    \"Sants-Montjuïc\",\n",
    "    \"Les Corts\",\n",
    "    \"Sarrià-Sant Gervasi\",\n",
    "    \"Gràcia\",\n",
    "    \"Horta-Guinardó\",\n",
    "    \"Nou Barris\",\n",
    "    \"Sant Andreu\",\n",
    "    \"Sant Martí\"\n",
    "]\n",
    "\n",
    "# Creamos una lista para almacenar enlaces de distritos\n",
    "link_distrito = []\n",
    "\n",
    "# URL base con el formato de Airbnb\n",
    "url_base = \"https://www.airbnb.es/s/{distrito}--Barcelona--España/homes?tab_id=home_tab&refinement_paths%5B%5D=%2Fhomes&monthly_start_date=2024-11-01&monthly_length=3&monthly_end_date=2025-02-01&price_filter_input_type=0&channel=EXPLORE&query={distrito}%2C%20Barcelona%2C%20España&date_picker_type=calendar&source=structured_search_input_header&search_type=autocomplete_click\"\n",
    "\n",
    "# Iterar sobre cada distrito y generar el enlace\n",
    "for distrito in distritos:\n",
    "    # Reemplazar espacios por %20 y caracteres especiales\n",
    "    distrito_url = distrito.replace(\" \", \"%20\").replace(\"à\", \"%C3%A0\").replace(\"ç\", \"%C3%A7\").replace(\"í\", \"%C3%AD\")\n",
    "    \n",
    "    # Formatear el enlace con el nombre del distrito\n",
    "    enlace = url_base.format(distrito=distrito_url)\n",
    "\n",
    "    link_distrito.append(enlace)\n",
    "\n",
    "# Lista vacía para almacenar los links de habitaciones\n",
    "links_to_scrapp = []\n",
    "\n",
    "# Accedemos a las paginas de cada distrito\n",
    "for distrito in tqdm(link_distrito, desc=\"Accediendo a distritos\"):\n",
    "    \n",
    "    driver.get(distrito)\n",
    "\n",
    "    # Pausa\n",
    "    sleep(3)\n",
    "\n",
    "    # Bucle para la paginación y extración\n",
    "    while len(links_to_scrapp) < 500:  # Limitar a 500 enlaces\n",
    "        try:\n",
    "            sleep(2)\n",
    "            # Obtener el código fuente de la página actual\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            # Extraer todos os enlaces de habitaciones en la página actual\n",
    "            link_habitacion = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"/rooms/\"]')\n",
    "\n",
    "            # Lista de urls target\n",
    "            links_target = []\n",
    "\n",
    "            # Recorrer todos los enlaces y filtrar los que contienen '/rooms'\n",
    "            for _ in link_habitacion:\n",
    "                link = _.get_attribute('href')\n",
    "                if link and '/rooms/' in link:\n",
    "                    links_target.append(link)\n",
    "\n",
    "            # Eliminar duplicados en la lista de enlaces\n",
    "            links_filtered = set(links_target)\n",
    "            print(f\"Número de links filtrados en la página: {len(links_filtered)}\")\n",
    "\n",
    "            # Añadimos los links en la lista de scrappeo\n",
    "            for url in links_filtered:\n",
    "                if len(links_to_scrapp) < 500:  # Limitar a 500 enlaces\n",
    "                    links_to_scrapp.append(url)\n",
    "                    print(f'Agregando URL: {url}')\n",
    "                else:\n",
    "                    break  # Interrompe o loop se já tiver 500 links\n",
    "\n",
    "            print(f\"Total de links extraídos hasta ahora: {len(links_to_scrapp)}\")\n",
    "\n",
    "            if len(links_to_scrapp) >= 500:  # Verifica se já atingiu 500 links\n",
    "                print('Total de links atingido. Parando a extração...')\n",
    "                break  # Para o loop de extração se atingir 500\n",
    "\n",
    "            # Pausa antes de hacer click en la próxima página\n",
    "            sleep(2)\n",
    "\n",
    "            # Buscar y hacer click en el botón \"Siguiente\"\n",
    "            next_button = driver.find_element(By.CSS_SELECTOR, 'a[aria-label=\"Siguiente\"]')\n",
    "            next_button.click()\n",
    "            print('Siguiente página...')\n",
    "\n",
    "        except Exception as e:\n",
    "            print('No se pudo cargar la siguiente página o no hay más páginas.')\n",
    "            print(e)\n",
    "            break\n",
    "\n",
    "# Imprimir a lista final de enlaces\n",
    "print(\"Extracción completa. Total de links:\", len(links_to_scrapp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer el texto de un elemento dado\n",
    "def extract_text(soup, tag, class_name, index=0, default=np.nan):\n",
    "    try:\n",
    "        elements = soup.find_all(tag, class_=class_name)\n",
    "        return elements[index].text.strip() if elements else default  # Retorna el texto del elemento especificado\n",
    "    except (AttributeError, IndexError):\n",
    "        return default\n",
    "\n",
    "# Diccionario para los campos a extraer, organizados en el orden deseado\n",
    "fields = {\n",
    "    \"Titles\": (\"span\", \"l1h825yc\", 1),  # Título del alojamiento\n",
    "    \"Host_name\": (\"div\", \"t1pxe1a4\", 0),  # Nombre del anfitrión\n",
    "    \"Property_types\": (\"h2\", \"hpipapi\", 0),  # Tipo de propiedad\n",
    "    \"Prices_per_night\": (\"span\", \"_11jcbg2\", 0),  # Precio por noche\n",
    "    \"Check_ins\": (\"div\", \"i1303y2k\", 0),  # Check-in\n",
    "    \"Check_outs\": (\"div\", \"i1303y2k\", 1),  # Check-out\n",
    "    \"Cleaning_fees\": (\"span\", \"_1k4xcdh\", 1),  # Tasa de limpieza\n",
    "    \"Location\": (\"div\", \"_152qbzi\", 0),  # Ubicación\n",
    "}\n",
    "\n",
    "# Limitar a 500 enlaces para la muestra\n",
    "sample_links = links_to_scrapp[:100]\n",
    "\n",
    "# Lista para almacenar los datos extraídos\n",
    "data_list = []\n",
    "\n",
    "# Iterar sobre cada enlace en la muestra\n",
    "for link in tqdm(sample_links, desc=\"Extrayendo datos\"):\n",
    "\n",
    "    # Asegurarse de que el enlace tenga el parámetro 'adults'\n",
    "    if 'adults' not in link:\n",
    "        link += '&adults=1'\n",
    "    else:\n",
    "        link += '?adults=1'    \n",
    "\n",
    "    driver.get(link)  # Acceder a la página\n",
    "    sleep(3)  # Espera para cargar la página\n",
    "\n",
    "    # Obtener el objeto BeautifulSoup\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    data = {'urls': link}  # Agregar la URL al diccionario de datos\n",
    "\n",
    "    # Extraer usando el diccionario y la función de extracción\n",
    "    data.update({field: extract_text(soup, *details) for field, details in fields.items()})\n",
    "\n",
    "    # Obtener las reseñas y calificaciones\n",
    "    reviews_data = driver.find_elements(By.XPATH, '//span[@aria-hidden=\"true\"]')\n",
    "    ratings_list = []\n",
    "    num_reviews_list = []\n",
    "    rating = num_reviews = np.nan  # Inicializa como NaN\n",
    "\n",
    "    print(f\"Total de reseñas encontradas: {len(reviews_data)}\")  # Debug: verificar el número de reseñas\n",
    "\n",
    "    # Procesar las reseñas\n",
    "    for review in reviews_data:\n",
    "        text = review.text\n",
    "        print(f\"Texto de la reseña: {text}\")  # Debug: ver el texto de las reseñas\n",
    "        if \"(\" in text:  # Verificar si el formato es el correcto\n",
    "            rating = text.split(' ')[0]\n",
    "            num_reviews = text.split('(')[1].replace(')', '')\n",
    "            ratings_list.append(rating)  # Añadir la calificación a la lista\n",
    "            num_reviews_list.append(num_reviews)  # Extraer el número de reseñas y quitar los paréntesis\n",
    "            break  # Salir del bucle después de encontrar el primer formato correcto\n",
    "\n",
    "    # Añadir calificaciones y número de reseñas al diccionario de datos\n",
    "    data['Ratings'] = rating\n",
    "    data['Num_reviews'] = num_reviews\n",
    "\n",
    "    # Extracción de detalles (camas, baños, etc.) usando el selector CSS\n",
    "    details = driver.find_elements(By.CSS_SELECTOR, \"div.o1kjrihn ol.lgx66tx\")\n",
    "    details_dict = {}\n",
    "    for detail in details:\n",
    "        detail_text = detail.text.strip()\n",
    "        # Lógica para identificar qué representa cada detalle\n",
    "        if \"camas\" in detail_text.lower():\n",
    "            details_dict['Beds'] = detail_text\n",
    "        elif \"dormitorios\" in detail_text.lower():\n",
    "            details_dict['Rooms'] = detail_text\n",
    "        elif \"baños\" in detail_text.lower():\n",
    "            details_dict['Baths'] = detail_text\n",
    "\n",
    "    # Agregar detalles al diccionario de datos\n",
    "    data.update(details_dict)\n",
    "\n",
    "    # Agregar el número máximo de huéspedes\n",
    "    n_guests = driver.find_elements(By.CSS_SELECTOR, \"div[data-plugin-in-point-id='POLICIES_DEFAULT'] div.i1303y2k span\")\n",
    "    guests_count = np.nan  # Valor predeterminado en caso de no encontrar\n",
    "    for guest in n_guests:\n",
    "        if 'huésped' in guest.text or 'huéspedes' in guest.text:\n",
    "            guests_count = guest.text.split()[1]  # Extraer solo el número de huéspedes\n",
    "            break  # Salir del bucle si se encuentra el número de huéspedes\n",
    "\n",
    "    # Añadir el número máximo de huéspedes al diccionario de datos\n",
    "    data['Maximum_guests'] = guests_count\n",
    "\n",
    "    # Añadir los datos extraídos a la lista\n",
    "    data_list.append(data)\n",
    "\n",
    "# Crear el DataFrame final con los datos extraídos\n",
    "df = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para no correr todo el codigo, podemos basearnos en este csv y corregir lo que esta mal\n",
    "df = pd.read_csv('df_testing_V2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
